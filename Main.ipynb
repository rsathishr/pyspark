{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7675f015-74f3-4fba-a4a8-645eca415bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### General Instructions <br /><br />\n",
    "\n",
    "- All relevant data sets are pre-loaded in the Jupyter Notebook environment.\n",
    "- The data set for the challenge will be provided when you start the assessment. Load them as required.\n",
    "- All files are in CSV format. Some files have headers and some files do not have headers\n",
    "- The files do not have headers\n",
    "  - CSV files: catgories.csv, customers.csv, departments.csv, orders.csv, order_items.csv, products.csv, customers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6406fb22-0205-4be1-8430-2ac82426be29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "**Fetch the top 10 categories with highest percentage of 'pending orders' in the year 2014**\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, categories.csv\n",
    "- An order is considered as pending order if the order status is either 'PENDING' or 'PENDING_PAYMENT' (in orders.csv)\n",
    "- Columns to be fetched are: **category, total_orders, pending_orders, percentage_pending_orders**\n",
    "- Round the **percentage_pending_orders** to one decimal place\n",
    "- Sort the data in the DESCENDING order of **percentage_pending_orders**\n",
    "- The output should have 10 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question1** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e12f3e-5d1d-487b-9644-47a4f04a637b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 06:59:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write your import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, countDistinct, sum, round, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"top 10 categories with highest percentage_pending_orders\").getOrCreate()\n",
    "\n",
    "# File paths \n",
    "categories_file = \"categories.csv\"\n",
    "customers_file = \"customers.csv\"\n",
    "departments_file = \"departments.csv\"\n",
    "order_items_file = \"order_items.csv\"\n",
    "orders_file = \"orders.csv\"\n",
    "products_file = \"products.csv\"\n",
    "\n",
    "#Write your code\n",
    "\n",
    "#Defining schema\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "customerSchema =  StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "])\n",
    "\n",
    "departmentSchema = StructType([\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "\n",
    "orders_2014 = orders.filter(year(col('order_date'))==2014)\n",
    "\n",
    "df_joined = pending_orders_2014.join(order_items,'order_id').join(products,'product_id').join(categories,'category_id')\n",
    "                                 \n",
    "result_1 = df_joined.groupBy('category').agg(countDistinct('order_id').alias('total_orders'),\n",
    "                                           sum(when(col('order_status').isin('PENDING_PAYMENT','PENDING'),1).otherwise(0)).alias('pending_orders')).withColumn('percentage_pending_orders',round((col('pending_orders') / col('total_orders'))*100,1)) \\\n",
    ".orderBy(col('percentage_pending_orders').desc()).limit(10)\n",
    "\n",
    "# result_1.show()\n",
    "\n",
    "#saving the output \n",
    "result_1.coalesce(1).write.mode('overwrite').option('header','true').csv('question1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b263d9-6dc0-4505-9ea7-0425743ecce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Question 2\n",
    "\n",
    "**Fetch two products with lowest 'total_sale_value' in 'Basketball', 'Football' and 'Soccer' categories in 'Sports' department in the year 2014. Consider orders with status as ‘COMPLETE’ only.** <br/><br/>\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "- Analyze and understand the data in all the datasets as per the ER diagram given above\n",
    "- ***total_sale_value*** is computed as the ***sum of total_price of all items*** in ***order_items*** dataset. \n",
    "- Consider only orders with ***COMPLETE*** as status in orders dataset.\n",
    "- Round the *total_sale_value* to nearest integer value. \n",
    "- Columns to be fetched are: **year, category, product, total_quantity, total_sale_value**\n",
    "- Sort the data in the ASCENDING order of category and ASCENDING order of total_sale_value.\n",
    "- The output should have 6 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question2** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1051fbcc-bf9e-4de9-a5aa-03b0d3e2bb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write your code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, countDistinct, sum, round, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"products with lowest total_sale_value\").getOrCreate()\n",
    "\n",
    "# Dataset to be used order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "departmentSchema = StructType([\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "departments = spark.read.csv(departments_file,schema = departmentSchema)\n",
    "\n",
    "# Filtering the order with completed status\n",
    "\n",
    "completed_orders = orders.filter((year(col('order_date'))=='2014') & (col('order_status')==\"COMPLETE\"))\n",
    "# completed_orders.show()\n",
    "\n",
    "df_joined = completed_orders.join(order_items, 'order_id') \\\n",
    "    .join(products,'product_id') \\\n",
    "    .join(categories, 'category_id') \\\n",
    "    .join(departments, 'department_id')\n",
    "\n",
    "\n",
    "df_filter = df_joined.filter((col('department') =='Sports') & \n",
    "                  (col('category').isin('Basketball', 'Football', 'Soccer')))\n",
    "agg_df = df_filter.groupBy(year(col('order_date')).alias('year'),\n",
    "                             col('category'),\n",
    "                             col('product')\n",
    "                          ).agg(sum('quantity').alias('total_quantity'),\n",
    "                                     round(sum('total_price')).alias('total_sale_value'))\n",
    "windSpec = Window.partitionBy('category').orderBy(col('total_sale_value').asc())\n",
    "\n",
    "result_2 = agg_df.withColumn('rn', row_number().over(windSpec)).filter(col('rn') <=2).drop('rn')\\\n",
    ".orderBy(col('category').asc(), col('total_sale_value').asc())\n",
    "\n",
    "result_2.coalesce(1).write.mode('overwrite').option('header','true').csv('question2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3ab049-d014-4720-b303-49d6e9595ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 3\n",
    "\n",
    "**Find top two worst performing states which recorded highest percentage of drop in 'total sales' in the year 2014 compared to year 2013 in 'Sports', 'Footwear', 'Fitness' and 'Golf' departments. Consider orders with status as ‘COMPLETE’ only.** <br /><br />\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv, customers.csv\n",
    "- Analyze and understand the data in all the datasets as per the ER diagram given above \n",
    "- **total_sales** is computed as **sum of total_price** in **order_items** dataset for that product.\n",
    "- Consider only COMPLETED orders (order_status in orders should be COMPLETE only)\n",
    "- Fetch the data related to top 2 states which recorder maximum percentage drop in total sale value in 2014 compared to 2013 in the specified departments.\n",
    "- Discard the data if there are no sales in either 2013 or 2014 in any state for any department\n",
    "- Round the **total_sales** values to nearest integer value\n",
    "- Round the **drop%** to two decimal places\n",
    "- Columns to be fetched are: **department, state, 2014_sales, 2013_sales, drop%**\n",
    "- The output should have 8 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question3** directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491368b4-13f1-4d6a-9221-ef850d1ac0d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write your code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9b9f6a-f893-482c-8357-e56cfef0a2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 4\n",
    "\n",
    "**Find out all the products that recorded more than 30% drop in total number of units sold (based on ‘quantity’ column in ‘order_items’ dataset) in year 2014 compared to year 2013 in 'Sports' and 'Fitness' departments. Consider orders with status as ‘COMPLETE’ only.** <br /><br />\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "- Quantity of each product is computed as **sum of quantity** in order_items datasets for that product\n",
    "- Columns to be fetched are: **department, product, 2013_qty, 2014_qty, drop%**\n",
    "    - 2013_qty: total quantity of the product in the year 2013\n",
    "    - 2014_qty: total quantity of the product in the year 2014\n",
    "    - drop%: drop in 2014 over 2013\n",
    "- Sort the data in the **ASCENDING order of department** and in the **DESCENDING order of drop%**.\n",
    "- Round the drop% to nearest integer value.\n",
    "- The output should have 4 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question4** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f049d107-b4e2-489a-9c98-1f337a159338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write your code"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1435047281523994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CS2-PySpark-PL3-Questions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
