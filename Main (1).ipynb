{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7675f015-74f3-4fba-a4a8-645eca415bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### General Instructions <br /><br />\n",
    "\n",
    "- All relevant data sets are pre-loaded in the Jupyter Notebook environment.\n",
    "- The data set for the challenge will be provided when you start the assessment. Load them as required.\n",
    "- All files are in CSV format. Some files have headers and some files do not have headers\n",
    "- The files do not have headers\n",
    "  - CSV files: catgories.csv, customers.csv, departments.csv, orders.csv, order_items.csv, products.csv, customers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6406fb22-0205-4be1-8430-2ac82426be29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "**Fetch the top 10 categories with highest percentage of 'pending orders' in the year 2014**\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, categories.csv\n",
    "- An order is considered as pending order if the order status is either 'PENDING' or 'PENDING_PAYMENT' (in orders.csv)\n",
    "- Columns to be fetched are: **category, total_orders, pending_orders, percentage_pending_orders**\n",
    "- Round the **percentage_pending_orders** to one decimal place\n",
    "- Sort the data in the DESCENDING order of **percentage_pending_orders**\n",
    "- The output should have 10 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question1** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e12f3e-5d1d-487b-9644-47a4f04a637b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write your import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, count, sum, round, row_number,countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"question1\").getOrCreate()\n",
    "\n",
    "# File paths \n",
    "categories_file = \"categories.csv\"\n",
    "customers_file = \"customers.csv\"\n",
    "departments_file = \"departments.csv\"\n",
    "order_items_file = \"order_items.csv\"\n",
    "orders_file = \"orders.csv\"\n",
    "products_file = \"products.csv\"\n",
    "\n",
    "#Write your code\n",
    "\n",
    "#Defining schema\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "\n",
    "orders_2014 = orders.filter(year(col('order_date'))==2014)\n",
    "\n",
    "df_joined = orders_2014.join(order_items,'order_id').join(products,'product_id').join(categories,'category_id')\n",
    "                                 \n",
    "result_1 = df_joined.groupBy('category').agg(countDistinct('order_id').alias('total_orders'),\n",
    "                                           sum(when(col('order_status').isin('PENDING_PAYMENT','PENDING'),1).otherwise(0)).alias('pending_orders')).withColumn('percentage_pending_orders',round((col('pending_orders') / col('total_orders'))*100,1)).orderBy(col('percentage_pending_orders').desc()).limit(10)\n",
    "# result_1.show()\n",
    "\n",
    "#saving the output \n",
    "result_1.coalesce(1).write.mode('overwrite').option('header','true').csv('question1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b263d9-6dc0-4505-9ea7-0425743ecce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Question 2\n",
    "\n",
    "**Fetch two products with lowest 'total_sale_value' in 'Basketball', 'Football' and 'Soccer' categories in 'Sports' department in the year 2014. Consider orders with status as ‘COMPLETE’ only.** <br/><br/>\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "- Analyze and understand the data in all the datasets as per the ER diagram given above\n",
    "- ***total_sale_value*** is computed as the ***sum of total_price of all items*** in ***order_items*** dataset. \n",
    "- Consider only orders with ***COMPLETE*** as status in orders dataset.\n",
    "- Round the *total_sale_value* to nearest integer value. \n",
    "- Columns to be fetched are: **year, category, product, total_quantity, total_sale_value**\n",
    "- Sort the data in the ASCENDING order of category and ASCENDING order of total_sale_value.\n",
    "- The output should have 6 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question2** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1051fbcc-bf9e-4de9-a5aa-03b0d3e2bb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write your code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, countDistinct, sum, round, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"question2\").getOrCreate()\n",
    "\n",
    "# Dataset to be used order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "departmentSchema = StructType([\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "departments = spark.read.csv(departments_file,schema = departmentSchema)\n",
    "\n",
    "# Filtering the order with completed status\n",
    "\n",
    "completed_orders = orders.filter((year(col('order_date'))=='2014') & (col('order_status')==\"COMPLETE\"))\n",
    "# completed_orders.show()\n",
    "\n",
    "# joining other dataframes\n",
    "df_joined = completed_orders.join(order_items, 'order_id') \\\n",
    "    .join(products,'product_id') \\\n",
    "    .join(categories, 'category_id') \\\n",
    "    .join(departments, 'department_id')\n",
    "\n",
    "#Filtering the data\n",
    "df_filter = df_joined.filter((col('department') =='Sports') & \n",
    "                  (col('category').isin('Basketball', 'Football', 'Soccer')))\n",
    "\n",
    "#Aggregation \n",
    "agg_df = df_filter.groupBy(year(col('order_date')).alias('year'),\n",
    "                             col('category'),\n",
    "                             col('product')\n",
    "                          ).agg(sum('quantity').alias('total_quantity'),\n",
    "                                     round(sum('total_price')).cast('int').alias('total_sale_value'))\n",
    "windSpec = Window.partitionBy('category').orderBy(col('total_sale_value').asc())\n",
    "\n",
    "result_2 = agg_df.withColumn('rn', row_number().over(windSpec)).filter(col('rn') <=2).drop('rn')\\\n",
    ".orderBy(col('category').asc(), col('total_sale_value').asc())\n",
    "\n",
    "#Saving the result to question2 path\n",
    "result_2.coalesce(1).write.mode('overwrite').option('header','true').csv('question2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3ab049-d014-4720-b303-49d6e9595ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 3\n",
    "\n",
    "**Find top two worst performing states which recorded highest percentage of drop in 'total sales' in the year 2014 compared to year 2013 in 'Sports', 'Footwear', 'Fitness' and 'Golf' departments. Consider orders with status as ‘COMPLETE’ only.** <br /><br />\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv, customers.csv\n",
    "- Analyze and understand the data in all the datasets as per the ER diagram given above \n",
    "- **total_sales** is computed as **sum of total_price** in **order_items** dataset for that product.\n",
    "- Consider only COMPLETED orders (order_status in orders should be COMPLETE only)\n",
    "- Fetch the data related to top 2 states which recorder maximum percentage drop in total sale value in 2014 compared to 2013 in the specified departments.\n",
    "- Discard the data if there are no sales in either 2013 or 2014 in any state for any department\n",
    "- Round the **total_sales** values to nearest integer value\n",
    "- Round the **drop%** to two decimal places\n",
    "- Columns to be fetched are: **department, state, 2014_sales, 2013_sales, drop%**\n",
    "- The output should have 8 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question3** directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491368b4-13f1-4d6a-9221-ef850d1ac0d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sales_2014` cannot be resolved. Did you mean one of the following? [`state`, `drop%`, `rn`, `2013_sales`, `2014_sales`].;\n'Project [department#14780, state#14790, 'sales_2014 AS 2014_sales#14988, 'sales_2013 AS 2013_sales#14989, drop%#14974]\n+- Filter (rn#14981 <= 2)\n   +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974, rn#14981]\n      +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974, rn#14981, rn#14981]\n         +- Window [row_number() windowspecdefinition(department#14780, drop%#14974 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#14981], [department#14780], [drop%#14974 DESC NULLS LAST]\n            +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974]\n               +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, round((((2013_sales#14964 - 2014_sales#14969) / 2013_sales#14964) * cast(100 as double)), 2) AS drop%#14974]\n                  +- Filter (isnotnull(2013_sales#14964) AND isnotnull(2014_sales#14969))\n                     +- Project [department#14780, state#14790, 2013_sales#14964, 2014#14955 AS 2014_sales#14969]\n                        +- Project [department#14780, state#14790, 2013#14954 AS 2013_sales#14964, 2014#14955]\n                           +- Project [department#14780, state#14790, __pivot_first(total_sales) AS `first(total_sales)`#14953[0] AS 2013#14954, __pivot_first(total_sales) AS `first(total_sales)`#14953[1] AS 2014#14955]\n                              +- Aggregate [department#14780, state#14790], [department#14780, state#14790, pivotfirst(year#14884, first(total_sales)#14947, 2013, 2014, 0, 0) AS __pivot_first(total_sales) AS `first(total_sales)`#14953]\n                                 +- Aggregate [department#14780, state#14790, year#14884], [department#14780, state#14790, year#14884, first(total_sales#14936, false) AS first(total_sales)#14947]\n                                    +- Aggregate [department#14780, state#14790, year#14884], [department#14780, state#14790, year#14884, round(sum(total_price#14755), 0) AS total_sales#14936]\n                                       +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791, year(order_date#14744) AS year#14884]\n                                          +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791]\n                                             +- Join Inner, (customer_id#14745 = customer_id#14783)\n                                                :- Filter department#14780 IN (Sports,Footwear,Fitness,Golf)\n                                                :  +- Project [department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780]\n                                                :     +- Join Inner, (department_id#14774 = department_id#14779)\n                                                :        :- Project [category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, department_id#14774, category#14775]\n                                                :        :  +- Join Inner, (category_id#14764 = category_id#14773)\n                                                :        :     :- Project [product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, category_id#14764, product#14765, type#14766, unit_price#14767]\n                                                :        :     :  +- Join Inner, (product_id#14753 = product_id#14763)\n                                                :        :     :     :- Project [order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, product_id#14753, quantity#14754, total_price#14755, unit_price#14756]\n                                                :        :     :     :  +- Join Inner, (order_id#14743 = order_id#14752)\n                                                :        :     :     :     :- Filter (order_status#14746 = COMPLETE)\n                                                :        :     :     :     :  +- Relation [order_id#14743,order_date#14744,customer_id#14745,order_status#14746] csv\n                                                :        :     :     :     +- Relation [order_item_id#14751,order_id#14752,product_id#14753,quantity#14754,total_price#14755,unit_price#14756] csv\n                                                :        :     :     +- Relation [product_id#14763,category_id#14764,product#14765,type#14766,unit_price#14767] csv\n                                                :        :     +- Relation [category_id#14773,department_id#14774,category#14775] csv\n                                                :        +- Relation [department_id#14779,department#14780] csv\n                                                +- Relation [customer_id#14783,first_name#14784,last_name#14785,email#14786,phone#14787,address#14788,city#14789,state#14790,zip#14791] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 93\u001b[0m\n\u001b[1;32m     86\u001b[0m sales_drop \u001b[38;5;241m=\u001b[39m sales_pivot\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2013_sales\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull() \u001b[38;5;241m&\u001b[39m col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2014_sales\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull()) \\\n\u001b[1;32m     87\u001b[0m \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m((col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2013_sales\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m-\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2014_sales\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m/\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2013_sales\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     89\u001b[0m windspec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[1;32m     92\u001b[0m result_3 \u001b[38;5;241m=\u001b[39m \u001b[43msales_drop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindspec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 93\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepartment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msales_2014\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2014_sales\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msales_2013\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2013_sales\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m result_3\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# # Total sales as per state,department,year\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# sales_df = df.groupBy(col('department'), col('state'), year(col('order_date')).alias('year')).agg(round(sum('total_price')).alias('total_sales'))\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# result_3.coalesce(1).write.mode('overwrite').option('header','true').csv('question3')\u001b[39;00m\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sales_2014` cannot be resolved. Did you mean one of the following? [`state`, `drop%`, `rn`, `2013_sales`, `2014_sales`].;\n'Project [department#14780, state#14790, 'sales_2014 AS 2014_sales#14988, 'sales_2013 AS 2013_sales#14989, drop%#14974]\n+- Filter (rn#14981 <= 2)\n   +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974, rn#14981]\n      +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974, rn#14981, rn#14981]\n         +- Window [row_number() windowspecdefinition(department#14780, drop%#14974 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#14981], [department#14780], [drop%#14974 DESC NULLS LAST]\n            +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, drop%#14974]\n               +- Project [department#14780, state#14790, 2013_sales#14964, 2014_sales#14969, round((((2013_sales#14964 - 2014_sales#14969) / 2013_sales#14964) * cast(100 as double)), 2) AS drop%#14974]\n                  +- Filter (isnotnull(2013_sales#14964) AND isnotnull(2014_sales#14969))\n                     +- Project [department#14780, state#14790, 2013_sales#14964, 2014#14955 AS 2014_sales#14969]\n                        +- Project [department#14780, state#14790, 2013#14954 AS 2013_sales#14964, 2014#14955]\n                           +- Project [department#14780, state#14790, __pivot_first(total_sales) AS `first(total_sales)`#14953[0] AS 2013#14954, __pivot_first(total_sales) AS `first(total_sales)`#14953[1] AS 2014#14955]\n                              +- Aggregate [department#14780, state#14790], [department#14780, state#14790, pivotfirst(year#14884, first(total_sales)#14947, 2013, 2014, 0, 0) AS __pivot_first(total_sales) AS `first(total_sales)`#14953]\n                                 +- Aggregate [department#14780, state#14790, year#14884], [department#14780, state#14790, year#14884, first(total_sales#14936, false) AS first(total_sales)#14947]\n                                    +- Aggregate [department#14780, state#14790, year#14884], [department#14780, state#14790, year#14884, round(sum(total_price#14755), 0) AS total_sales#14936]\n                                       +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791, year(order_date#14744) AS year#14884]\n                                          +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791]\n                                             +- Join Inner, (customer_id#14745 = customer_id#14783)\n                                                :- Filter department#14780 IN (Sports,Footwear,Fitness,Golf)\n                                                :  +- Project [department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780]\n                                                :     +- Join Inner, (department_id#14774 = department_id#14779)\n                                                :        :- Project [category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, department_id#14774, category#14775]\n                                                :        :  +- Join Inner, (category_id#14764 = category_id#14773)\n                                                :        :     :- Project [product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, category_id#14764, product#14765, type#14766, unit_price#14767]\n                                                :        :     :  +- Join Inner, (product_id#14753 = product_id#14763)\n                                                :        :     :     :- Project [order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, product_id#14753, quantity#14754, total_price#14755, unit_price#14756]\n                                                :        :     :     :  +- Join Inner, (order_id#14743 = order_id#14752)\n                                                :        :     :     :     :- Filter (order_status#14746 = COMPLETE)\n                                                :        :     :     :     :  +- Relation [order_id#14743,order_date#14744,customer_id#14745,order_status#14746] csv\n                                                :        :     :     :     +- Relation [order_item_id#14751,order_id#14752,product_id#14753,quantity#14754,total_price#14755,unit_price#14756] csv\n                                                :        :     :     +- Relation [product_id#14763,category_id#14764,product#14765,type#14766,unit_price#14767] csv\n                                                :        :     +- Relation [category_id#14773,department_id#14774,category#14775] csv\n                                                :        +- Relation [department_id#14779,department#14780] csv\n                                                +- Relation [customer_id#14783,first_name#14784,last_name#14785,email#14786,phone#14787,address#14788,city#14789,state#14790,zip#14791] csv\n"
     ]
    }
   ],
   "source": [
    "#Write your code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, countDistinct, sum, round, row_number, first\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"question3\").getOrCreate()\n",
    "\n",
    "# Dataset to be used order_items.csv, orders.csv, products.csv, departments.csv, categories.csv, customers.csv\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "departmentSchema = StructType([\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "customerSchema =  StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "departments = spark.read.csv(departments_file,schema = departmentSchema)\n",
    "customers = spark.read.csv(customers_file,schema = customerSchema)\n",
    "\n",
    "completed_orders = orders.filter(col('order_status')==\"COMPLETE\")\n",
    "\n",
    "df = completed_orders.join(order_items, 'order_id') \n",
    "df_join = df.join(products, 'product_id') \\\n",
    "                    .join(categories, 'category_id') \\\n",
    "                    .join(departments, 'department_id') \\\n",
    "                    \n",
    "\n",
    "target_dept = ['Sports','Footwear','Fitness','Golf']\n",
    "filter_dpet = df_join.filter(col('department').isin(target_dept))\n",
    "\n",
    "df_join = filter_dpet.join(customers, 'customer_id')\n",
    "\n",
    "sales_df = df_join.withColumn('year',year('order_date'))\n",
    "\n",
    "sales_agg = sales_df.groupBy('department','state','year').agg(round(sum('total_price')).alias('total_sales'))\n",
    "\n",
    "sales_pivot = sales_agg.groupBy('department','state').pivot('year',[2013, 2014]).agg(first('total_sales')).withColumnRenamed('2013','2013_sales').withColumnRenamed('2014','2014_sales')\n",
    "\n",
    "\n",
    "sales_drop = sales_pivot.filter(col('2013_sales').isNotNull() & col('2014_sales').isNotNull()) \\\n",
    ".withColumn(\"drop%\", round((col('2013_sales')-col('2014_sales'))/col('2013_sales')*100,2))\n",
    "\n",
    "windspec = Window.partitionBy('department').orderBy(col('drop%').desc())\n",
    "\n",
    "\n",
    "result_3 = sales_drop.withColumn('rn', row_number().over(windspec)).filter(col('rn')<=2) \\\n",
    "            .select('department','state',col('sales_2014').alias('2014_sales'),col('sales_2013').alias('2013_sales'),col('drop%'))\n",
    "result_3.show()\n",
    "            \n",
    "\n",
    "# # Total sales as per state,department,year\n",
    "# sales_df = df.groupBy(col('department'), col('state'), year(col('order_date')).alias('year')).agg(round(sum('total_price')).alias('total_sales'))\n",
    "\n",
    "\n",
    "# # Filter 2013 sales & 2014 sales\n",
    "# sales_2013 = sales_df.filter(col('year')=='2013').select('department','state', col('total_sales').alias('sales_2013'))\n",
    "# sales_2014 = sales_df.filter(col('year')=='2014').select('department','state', col('total_sales').alias('sales_2014'))\n",
    "\n",
    "# df_joined = sales_2013.join(sales_2014, ['department','state'],'inner')\n",
    "\n",
    "# result_df = df_joined.withColumn('drop%', round(((col('sales_2013')-col('sales_2014'))/col('sales_2013'))*100, 2) )\n",
    "\n",
    "# windspec = Window.partitionBy('department').orderBy(col('drop%').desc())\n",
    "\n",
    "# result_3 = result_df.withColumn('rn', row_number().over(windspec)).filter(col('rn')<=2) \\\n",
    "#             .select('department','state',col('sales_2014').alias('2014_sales'),col('sales_2013').alias('2013_sales'),col('drop%'))\\\n",
    "# .orderBy(col('department'), col(\"drop%\").desc())\n",
    "\n",
    "# #Saving the result in question3 path\n",
    "\n",
    "# result_3.coalesce(1).write.mode('overwrite').option('header','true').csv('question3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `department_name` cannot be resolved. Did you mean one of the following? [`department_id`, `department`, `first_name`, `last_name`, `order_date`].;\n'Aggregate ['department_name, state#14790, year#15067], ['department_name, state#14790, year#15067, round(sum(total_price#14755), 0) AS total_sales#15119]\n+- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791, year(order_date#14744) AS year#15067]\n   +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791]\n      +- Join Inner, (customer_id#14745 = customer_id#14783)\n         :- Filter department#14780 IN (Sports,Footwear,Fitness,Golf)\n         :  +- Project [department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780]\n         :     +- Join Inner, (department_id#14774 = department_id#14779)\n         :        :- Project [category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, department_id#14774, category#14775]\n         :        :  +- Join Inner, (category_id#14764 = category_id#14773)\n         :        :     :- Project [product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, category_id#14764, product#14765, type#14766, unit_price#14767]\n         :        :     :  +- Join Inner, (product_id#14753 = product_id#14763)\n         :        :     :     :- Project [order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, product_id#14753, quantity#14754, total_price#14755, unit_price#14756]\n         :        :     :     :  +- Join Inner, (order_id#14743 = order_id#14752)\n         :        :     :     :     :- Filter (order_status#14746 = COMPLETE)\n         :        :     :     :     :  +- Relation [order_id#14743,order_date#14744,customer_id#14745,order_status#14746] csv\n         :        :     :     :     +- Relation [order_item_id#14751,order_id#14752,product_id#14753,quantity#14754,total_price#14755,unit_price#14756] csv\n         :        :     :     +- Relation [product_id#14763,category_id#14764,product#14765,type#14766,unit_price#14767] csv\n         :        :     +- Relation [category_id#14773,department_id#14774,category#14775] csv\n         :        +- Relation [department_id#14779,department#14780] csv\n         +- Relation [customer_id#14783,first_name#14784,last_name#14785,email#14786,phone#14787,address#14788,city#14789,state#14790,zip#14791] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m sales_with_year \u001b[38;5;241m=\u001b[39m sales_with_state\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, year(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 9. Aggregate sales per department, state, year\u001b[39;00m\n\u001b[1;32m     23\u001b[0m sales_agg \u001b[38;5;241m=\u001b[39m \u001b[43msales_with_year\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdepartment_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_price\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_sales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 10. Pivot to compare 2013 vs 2014\u001b[39;00m\n\u001b[1;32m     27\u001b[0m sales_pivot \u001b[38;5;241m=\u001b[39m sales_agg\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mpivot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m2013\u001b[39m, \u001b[38;5;241m2014\u001b[39m]) \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39magg(first(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2013\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2013_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2014\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2014_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/spark/spark-3.5.0/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `department_name` cannot be resolved. Did you mean one of the following? [`department_id`, `department`, `first_name`, `last_name`, `order_date`].;\n'Aggregate ['department_name, state#14790, year#15067], ['department_name, state#14790, year#15067, round(sum(total_price#14755), 0) AS total_sales#15119]\n+- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791, year(order_date#14744) AS year#15067]\n   +- Project [customer_id#14745, department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780, first_name#14784, last_name#14785, email#14786, phone#14787, address#14788, city#14789, state#14790, zip#14791]\n      +- Join Inner, (customer_id#14745 = customer_id#14783)\n         :- Filter department#14780 IN (Sports,Footwear,Fitness,Golf)\n         :  +- Project [department_id#14774, category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, category#14775, department#14780]\n         :     +- Join Inner, (department_id#14774 = department_id#14779)\n         :        :- Project [category_id#14764, product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, product#14765, type#14766, unit_price#14767, department_id#14774, category#14775]\n         :        :  +- Join Inner, (category_id#14764 = category_id#14773)\n         :        :     :- Project [product_id#14753, order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, quantity#14754, total_price#14755, unit_price#14756, category_id#14764, product#14765, type#14766, unit_price#14767]\n         :        :     :  +- Join Inner, (product_id#14753 = product_id#14763)\n         :        :     :     :- Project [order_id#14743, order_date#14744, customer_id#14745, order_status#14746, order_item_id#14751, product_id#14753, quantity#14754, total_price#14755, unit_price#14756]\n         :        :     :     :  +- Join Inner, (order_id#14743 = order_id#14752)\n         :        :     :     :     :- Filter (order_status#14746 = COMPLETE)\n         :        :     :     :     :  +- Relation [order_id#14743,order_date#14744,customer_id#14745,order_status#14746] csv\n         :        :     :     :     +- Relation [order_item_id#14751,order_id#14752,product_id#14753,quantity#14754,total_price#14755,unit_price#14756] csv\n         :        :     :     +- Relation [product_id#14763,category_id#14764,product#14765,type#14766,unit_price#14767] csv\n         :        :     +- Relation [category_id#14773,department_id#14774,category#14775] csv\n         :        +- Relation [department_id#14779,department#14780] csv\n         +- Relation [customer_id#14783,first_name#14784,last_name#14785,email#14786,phone#14787,address#14788,city#14789,state#14790,zip#14791] csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Filter only COMPLETED orders\n",
    "orders_filtered = orders.filter(col(\"order_status\") == \"COMPLETE\")\n",
    "\n",
    "# 4. Join orders with order_items\n",
    "order_items_joined = orders_filtered.join(order_items, \"order_id\")\n",
    "\n",
    "# 5. Join with products → categories → departments\n",
    "products_joined = order_items_joined.join(products, \"product_id\") \\\n",
    "    .join(categories, \"category_id\") \\\n",
    "    .join(departments, \"department_id\")\n",
    "\n",
    "# 6. Filter target departments\n",
    "target_departments = [\"Sports\", \"Footwear\", \"Fitness\", \"Golf\"]\n",
    "filtered_depts = products_joined.filter(col(\"department\").isin(target_departments))\n",
    "\n",
    "# 7. Join with customers to get state\n",
    "sales_with_state = filtered_depts.join(customers, \"customer_id\")\n",
    "\n",
    "# 8. Extract year from order_date\n",
    "sales_with_year = sales_with_state.withColumn(\"year\", year(\"order_date\"))\n",
    "\n",
    "# 9. Aggregate sales per department, state, year\n",
    "sales_agg = sales_with_year.groupBy(\"department_name\", \"state\", \"year\") \\\n",
    "    .agg(round(sum(\"total_price\")).alias(\"total_sales\"))\n",
    "\n",
    "# 10. Pivot to compare 2013 vs 2014\n",
    "sales_pivot = sales_agg.groupBy(\"department\", \"state\") \\\n",
    "    .pivot(\"year\", [2013, 2014]) \\\n",
    "    .agg(first(\"total_sales\")) \\\n",
    "    .withColumnRenamed(\"2013\", \"2013_sales\") \\\n",
    "    .withColumnRenamed(\"2014\", \"2014_sales\")\n",
    "\n",
    "# 11. Calculate drop %\n",
    "sales_drop = sales_pivot.filter(col(\"2013_sales\").isNotNull() & col(\"2014_sales\").isNotNull()) \\\n",
    "    .withColumn(\"drop%\", round((col(\"2013_sales\") - col(\"2014_sales\")) / col(\"2013_sales\") * 100, 2))\n",
    "\n",
    "# 12. Rank states by drop% per department and select top 2\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"drop%\").desc())\n",
    "\n",
    "top2_states = sales_drop.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 2) \\\n",
    "    .select(\"department\", \"state\", \"2014_sales\", \"2013_sales\", \"drop%\")\n",
    "\n",
    "# 13. Save output as CSV\n",
    "# top2_states.coalesce(1).write.csv(\"question3/output.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 14. Show result in console\n",
    "top2_states.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a9b9f6a-f893-482c-8357-e56cfef0a2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Question 4\n",
    "\n",
    "**Find out all the products that recorded more than 30% drop in total number of units sold (based on ‘quantity’ column in ‘order_items’ dataset) in year 2014 compared to year 2013 in 'Sports' and 'Fitness' departments. Consider orders with status as ‘COMPLETE’ only.** <br /><br />\n",
    "\n",
    "- Datasets to be used to solve this task are\n",
    "  - order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "- Quantity of each product is computed as **sum of quantity** in order_items datasets for that product\n",
    "- Columns to be fetched are: **department, product, 2013_qty, 2014_qty, drop%**\n",
    "    - 2013_qty: total quantity of the product in the year 2013\n",
    "    - 2014_qty: total quantity of the product in the year 2014\n",
    "    - drop%: drop in 2014 over 2013\n",
    "- Sort the data in the **ASCENDING order of department** and in the **DESCENDING order of drop%**.\n",
    "- Round the drop% to nearest integer value.\n",
    "- The output should have 4 rows, excluding the header\n",
    "- Save the output as a single CSV file with header in **question4** directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f049d107-b4e2-489a-9c98-1f337a159338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write your code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType,StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col,year,when, countDistinct, sum, round, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"question4\").getOrCreate()\n",
    "\n",
    "# Dataset to be used order_items.csv, orders.csv, products.csv, departments.csv, categories.csv\n",
    "orderSchema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "orderItemSchema = StructType([\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total_price\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "categoriesSchema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "departmentSchema = StructType([\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Reading the csv files\n",
    "orders = spark.read.csv(orders_file,schema = orderSchema)\n",
    "order_items = spark.read.csv(order_items_file,schema = orderItemSchema)\n",
    "products = spark.read.csv(products_file,schema = productSchema)\n",
    "categories = spark.read.csv(categories_file,schema = categoriesSchema)\n",
    "departments = spark.read.csv(departments_file,schema = departmentSchema)\n",
    "\n",
    "#completed orders\n",
    "completed_orders = orders.filter(col('order_status')==\"COMPLETE\")\n",
    "\n",
    "#Joining dataframes\n",
    "df = completed_orders.join(order_items, 'order_id') \\\n",
    "                    .join(products, 'product_id') \\\n",
    "                    .join(categories, 'category_id') \\\n",
    "                    .join(departments, 'department_id') \n",
    "df = df.filter(col('department').isin('Sports','Fitness'))\n",
    "\n",
    "#Aggregating the dataframe\n",
    "df_agg = df.groupBy('department', 'product',year('order_date').alias('year')).agg(sum('quantity').alias('total_qty'))\n",
    "\n",
    "# Split 2013 & 2014 records\n",
    "\n",
    "qty_2013 = df_agg.filter(col('year')== 2013).select('department','product',col('total_qty').alias('2013_qty'))\n",
    "qty_2014 = df_agg.filter(col('year')== 2014).select('department','product',col('total_qty').alias('2014_qty'))\n",
    "\n",
    "df_joined = qty_2013.join(qty_2014, ['department','product'],'inner')\n",
    "\n",
    "# Filtering drop %\n",
    "drop_df = df_joined.withColumn('drop%',\n",
    "                               round(((col('2013_qty')-col('2014_qty'))/col('2013_qty'))*100).cast('int'))\n",
    "result_4 = drop_df.filter(col('drop%') > 30).orderBy(col('department').asc(), col('drop%').desc())\n",
    "\n",
    "result_4.coalesce(1).write.mode('overwrite').option('header','true').csv('question4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1435047281523994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CS2-PySpark-PL3-Questions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
